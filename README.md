# 🧠 Object Recognition / Classification using Computer Vision in Machine Learning

This project focuses on **Object Recognition and Image Classification** using multiple **deep learning architectures** — from traditional CNNs to modern Transformers — trained and evaluated on the **CIFAR-10 dataset**.

---

## 🚀 **Project Overview**

The goal of this research is to compare and analyze the performance of various computer vision models to understand how **model architecture evolution** (from CNN → EfficientNet → ConvNeXt → Vision Transformer) impacts accuracy, efficiency, and interpretability.

| Model | Type | Description | Accuracy (%) |
|:------|:------|:------------|:-------------|
| ResNet18 | Traditional CNN | Baseline convolutional model for feature extraction | 83.26 |
| EfficientNet-B0 | Modern CNN | Parameter-efficient CNN with compound scaling | 87.45 |
| ConvNeXt-Tiny | Next-Gen CNN | CNN architecture redesigned to match transformer performance | 88.97 |
| ViT-B/16 | Transformer | Vision Transformer with self-attention for image classification | **96.15** |

---

## 🧱 **Project Structure**

<pre> ```plaintext object-recog-project/ │ ├── src/ │ ├── train.py # Training + validation pipeline │ ├── datasets.py # CIFAR-10 dataloaders and preprocessing │ ├── models/ │ │ ├── resnet.py # ResNet model │ │ ├── efficientnet.py # EfficientNet model │ │ ├── convnext.py # ConvNeXt model │ │ └── vit.py # Vision Transformer (ViT) │ ├── utils.py # Accuracy, checkpoint, seed, etc. │ ├── config.py # Directory paths and constants │ └── plot_results.py # Accuracy & loss trend visualization │ ├── checkpoints/ # Saved model weights per epoch ├── results/ # Evaluation results and comparison plots └── README.md ``` </pre>

---

## ⚙️ **Setup & Installation**

### 1️⃣ Clone the repository
```bash
git clone https://github.com/your-username/object-recog-project.git
cd object-recog-project

### 2️⃣ Create a virtual environment
python -m venv venv
venv\Scripts\activate   # On Windows
# or
source venv/bin/activate  # On macOS/Linux

### 3️⃣ Install dependencies 
pip install -r requirements.txt

---

🧩 Dataset

The project uses CIFAR-10, a standard benchmark dataset for image classification:

-60,000 images (32×32 pixels)

-10 classes (airplane, car, bird, cat, etc.)

-Automatically downloaded when training starts.

---

🏋️‍♂️ Training Command Examples

Train Vision Transformer (ViT-B/16) on CIFAR-10:
python -u -m src.train --model vit_b_16 --dataset cifar10 --epochs 10 --batch-size 16 --lr 0.001 --save-dir checkpoints/vit_cifar10 --device cuda

---

Train ResNet18:
python -u -m src.train --model resnet18 --dataset cifar10 --epochs 10 --batch-size 64 --lr 0.01 --save-dir checkpoints/resnet_cifar10

---

📊 Results & Comparison

After training, accuracy results were as follows:

Model	            Accuracy(%)	  Test Loss	  Epochs	Device
ResNet18	          83.26	     0.5051	       10	     CPU
EfficientNet-B0	    87.45	     0.41	         10	     GPU
ConvNeXt-Tiny	      88.97	     0.33	         10	     GPU
ViT-B/16	          96.15	     0.1491	       10	     GPU

--- 

📈 Accuracy Improvement Trend

The graph (generated by src/plot_results.py) visualizes how performance increases with model evolution:

ResNet18  →  EfficientNet-B0  →  ConvNeXt-Tiny  →  ViT-B/16
     ↑             ↑                 ↑                  ↑
  83.26%        87.45%           88.97%             96.15%

To generate the comparison plot:
python -u src/plot_results.py

A graph will be saved automatically to:
results/model_comparison.png

---

🧠 Key Insights

Transformers dominate: Vision Transformers (ViT) significantly outperform CNNs on image classification.

-Accuracy jump : From 83% → 96% using architectural evolution.

-Training trade-off : ViT models require more compute but learn global dependencies more effectively.

-Explainability : Next phase includes Grad-CAM & attention visualization to understand learned features.

---

🧾 References

-Dosovitskiy et al., An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT), 2021.

-Liu et al., ConvNeXt: A ConvNet for the 2020s, 2022.

-Tan & Le, EfficientNet: Rethinking Model Scaling for CNNs, 2019.

-He et al., Deep Residual Learning for Image Recognition (ResNet), 2016.

-CIFAR-10 Dataset: https://www.cs.toronto.edu/~kriz/cifar.html
