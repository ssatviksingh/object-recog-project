# ğŸ§  Object Recognition / Classification using Computer Vision in Machine Learning

This project focuses on **Object Recognition and Image Classification** using multiple **deep learning architectures** â€” from traditional CNNs to modern Transformers â€” trained and evaluated on the **CIFAR-10 dataset**.

---

## ğŸš€ **Project Overview**

The goal of this research is to compare and analyze the performance of various computer vision models to understand how **model architecture evolution** (from CNN â†’ EfficientNet â†’ ConvNeXt â†’ Vision Transformer) impacts accuracy, efficiency, and interpretability.

| Model | Type | Description | Accuracy (%) |
|:------|:------|:------------|:-------------|
| ResNet18 | Traditional CNN | Baseline convolutional model for feature extraction | 83.26 |
| EfficientNet-B0 | Modern CNN | Parameter-efficient CNN with compound scaling | 87.45 |
| ConvNeXt-Tiny | Next-Gen CNN | CNN architecture redesigned to match transformer performance | 88.97 |
| ViT-B/16 | Transformer | Vision Transformer with self-attention for image classification | **96.15** |

---

## ğŸ§± **Project Structure**

<pre> ```plaintext object-recog-project/ â”‚ â”œâ”€â”€ src/ â”‚ â”œâ”€â”€ train.py # Training + validation pipeline â”‚ â”œâ”€â”€ datasets.py # CIFAR-10 dataloaders and preprocessing â”‚ â”œâ”€â”€ models/ â”‚ â”‚ â”œâ”€â”€ resnet.py # ResNet model â”‚ â”‚ â”œâ”€â”€ efficientnet.py # EfficientNet model â”‚ â”‚ â”œâ”€â”€ convnext.py # ConvNeXt model â”‚ â”‚ â””â”€â”€ vit.py # Vision Transformer (ViT) â”‚ â”œâ”€â”€ utils.py # Accuracy, checkpoint, seed, etc. â”‚ â”œâ”€â”€ config.py # Directory paths and constants â”‚ â””â”€â”€ plot_results.py # Accuracy & loss trend visualization â”‚ â”œâ”€â”€ checkpoints/ # Saved model weights per epoch â”œâ”€â”€ results/ # Evaluation results and comparison plots â””â”€â”€ README.md ``` </pre>

---

## âš™ï¸ **Setup & Installation**

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/your-username/object-recog-project.git
cd object-recog-project

### 2ï¸âƒ£ Create a virtual environment
python -m venv venv
venv\Scripts\activate   # On Windows
# or
source venv/bin/activate  # On macOS/Linux

### 3ï¸âƒ£ Install dependencies 
pip install -r requirements.txt

---

ğŸ§© Dataset

The project uses CIFAR-10, a standard benchmark dataset for image classification:

-60,000 images (32Ã—32 pixels)

-10 classes (airplane, car, bird, cat, etc.)

-Automatically downloaded when training starts.

---

ğŸ‹ï¸â€â™‚ï¸ Training Command Examples

Train Vision Transformer (ViT-B/16) on CIFAR-10:
python -u -m src.train --model vit_b_16 --dataset cifar10 --epochs 10 --batch-size 16 --lr 0.001 --save-dir checkpoints/vit_cifar10 --device cuda

---

Train ResNet18:
python -u -m src.train --model resnet18 --dataset cifar10 --epochs 10 --batch-size 64 --lr 0.01 --save-dir checkpoints/resnet_cifar10

---

ğŸ“Š Results & Comparison

After training, accuracy results were as follows:

Model	            Accuracy(%)	  Test Loss	  Epochs	Device
ResNet18	          83.26	     0.5051	       10	     CPU
EfficientNet-B0	    87.45	     0.41	         10	     GPU
ConvNeXt-Tiny	      88.97	     0.33	         10	     GPU
ViT-B/16	          96.15	     0.1491	       10	     GPU

--- 

ğŸ“ˆ Accuracy Improvement Trend

The graph (generated by src/plot_results.py) visualizes how performance increases with model evolution:

ResNet18  â†’  EfficientNet-B0  â†’  ConvNeXt-Tiny  â†’  ViT-B/16
     â†‘             â†‘                 â†‘                  â†‘
  83.26%        87.45%           88.97%             96.15%

To generate the comparison plot:
python -u src/plot_results.py

A graph will be saved automatically to:
results/model_comparison.png

---

ğŸ§  Key Insights

Transformers dominate: Vision Transformers (ViT) significantly outperform CNNs on image classification.

-Accuracy jump : From 83% â†’ 96% using architectural evolution.

-Training trade-off : ViT models require more compute but learn global dependencies more effectively.

-Explainability : Next phase includes Grad-CAM & attention visualization to understand learned features.

---

ğŸ§¾ References

-Dosovitskiy et al., An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT), 2021.

-Liu et al., ConvNeXt: A ConvNet for the 2020s, 2022.

-Tan & Le, EfficientNet: Rethinking Model Scaling for CNNs, 2019.

-He et al., Deep Residual Learning for Image Recognition (ResNet), 2016.

-CIFAR-10 Dataset: https://www.cs.toronto.edu/~kriz/cifar.html
